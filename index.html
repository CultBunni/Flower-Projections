
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/pose"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection"></script>
<script>
 
bgColor: pink
   
  
  let gif, detector, video
  
  
  async function addVideo() {
    const video = document.createElement('video')
    video.setAttribute('autoplay', '')
    video.setAttribute('muted', '')
    video.setAttribute('playsinline', '')
    document.body.appendChild(video)
    const stream = await navigator.mediaDevices.getUserMedia({video: true})
    video.srcObject = stream    
    return video
  }

  async function setupModel () {
    
    const model = poseDetection.SupportedModels.BlazePose
    const detectorConfig = {
      runtime: 'mediapipe',
      solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/pose'
    }
    
    const detector = await poseDetection.createDetector(model, detectorConfig)
    return detector
  }
  
  
  async function draw () {
    // we first use the detector AI function to predict our "pose" based on the video frame
		const poses = await detector.estimatePoses(video)
    if (poses.length > 0) { // if the AI detects poses...
      // we grab the first keypoint from the first pose detected, ie our "nose"
      // we can see which keypoints refer to which part of the pose in the docs:
      // https://github.com/tensorflow/tfjs-models/tree/master/pose-detection#blazepose-keypoints-used-in-mediapipe-blazepose
    	const nose = poses[0].keypoints[0]
      // nose object looks like: { x: number, y: number, z: number, score: number, name: "nose" }
      console.log(nose) 
    	gif.style.left = nose.x - gif.width / 2.5 + 'px'
      gif.style.top = nose.y - gif.height / 2.5 + 'px'
    }
    requestAnimationFrame(draw)
  }


  async function setup () {
    // we create a gif element
    gif = document.createElement('img')
    gif.src = 'pink_glitter_flower.gif'
    gif.style.position = 'absolute'
    document.body.appendChild(gif)
    
  
    video = await addVideo()
    // then we create our AI function
  	detector = await setupModel()
    // then we log it to make sure the model loaded correctly
    console.log('detector ready', detector)
  
    // then we wait a second before starting our animation loop
    setTimeout(draw, 1000)  // this is a hack!
    // ...to get around the "race conditions" issue
  }

  window.addEventListener('load', setup)

</script>